# Got + Cheerio â€“ Zero to Advanced HTTP & Web Scraping Guide

Complete reference for modern HTTP requests and DOM parsing in TypeScript/Node.js. Copy-paste ready, production-tested.

---

## Table of Contents
1. [Got â€“ HTTP Requests](#got--http-requests)
2. [Cheerio â€“ Data Extraction](#cheerio--data-extraction)
3. [Real-World Examples](#real-world-examples)
4. [Cheat Sheets](#cheat-sheets)

---

## Got â€“ HTTP Requests

### Why Got?
- 100% TypeScript support with built-in types
- Promise-based, human-friendly API
- Built-in retry, timeout, redirect, streaming
- Better defaults than Axios
- Hooks, caching, proxy support

### Installation
```bash
npm install got
# Types are built-in (no @types/got needed)
```

### Basic Requests

```ts
import got from 'got';

// Simple GET
const res = await got('https://httpbin.org/json').json();
console.log(res);

// POST with JSON
await got.post('https://httpbin.org/post', {
  json: { hello: 'world' },
});

// GET with custom headers
await got('https://api.github.com/users/octocat', {
  headers: { 'User-Agent': 'my-scraper/1.0' }
});

// GET as text
const html = await got(url).text();

// GET as buffer
const buffer = await got(url).buffer();
```

### Common Options (99% of Use Cases)

```ts
await got(url, {
  timeout: { request: 10000 },          // 10s total timeout
  retry: { limit: 3 },                  // retry 3 times on failure
  followRedirect: true,                 // follow redirects
  throwHttpErrors: false,               // don't throw on 4xx/5xx
  headers: {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept': 'text/html,application/xhtml+xml',
    'Accept-Language': 'en-US,en;q=0.9',
  },
});
```

### Pro-Level Got Instance (Recommended for Scraping)

```ts
import got, { Got } from 'got';
import { HttpsProxyAgent } from 'https-proxy-agent';

const scraper = got.extend({
  timeout: { request: 15000 },
  retry: { limit: 3, methods: ['GET', 'POST'] },
  followRedirect: true,
  headers: {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
  },
  hooks: {
    beforeRequest: [
      options => {
        console.log(`â†’ ${options.method} ${options.url}`);
      }
    ],
    afterResponse: [
      response => {
        console.log(`â† ${response.statusCode} ${response.url}`);
        return response;
      }
    ]
  }
});

// Reuse everywhere
const html = await scraper.get('https://example.com').text();
```

### Proxy Support

```ts
import { HttpsProxyAgent } from 'https-proxy-agent';

const proxy = 'http://user:pass@123.45.67.89:8000';
const agent = new HttpsProxyAgent(proxy);

await got(url, {
  agent: { https: agent }
});

// Or in extended instance
const scraperWithProxy = got.extend({
  agent: { https: new HttpsProxyAgent(proxy) }
});
```

### Random User-Agent + Proxy Rotation

```ts
const userAgents = [
  'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',
  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)...',
  // ... add more
];
const proxies = ['http://ip1:port', 'http://ip2:port'];

let i = 0;
const scraper = got.extend({
  headers: { 
    'User-Agent': () => userAgents[i % userAgents.length] 
  },
  agent: { 
    https: () => new HttpsProxyAgent(proxies[i++ % proxies.length]) 
  }
});
```

### Streaming (For Large Files)

```ts
import { pipeline } from 'stream/promises';
import fs from 'fs';

await pipeline(
  got.stream('https://huge-file.com/data.zip'),
  fs.createWriteStream('data.zip')
);
```

### JSON Mode with Types

```ts
interface User { 
  login: string; 
  id: number 
}

const user = await got('https://api.github.com/users/octocat')
  .json<User>();
```

### Error Handling Best Practices

```ts
try {
  const res = await scraper.get(url);
} catch (err: any) {
  if (err.response?.statusCode === 429) {
    console.log('Rate limited! Waiting...');
    await new Promise(r => setTimeout(r, 5000));
  }
  if (err.code === 'ETIMEDOUT') {
    console.log('Timeout â€“ retrying with proxy');
  }
  if (err.code === 'ECONNREFUSED') {
    console.log('Connection refused â€“ proxy might be dead');
  }
}
```

### Pagination Helper

```ts
async function* fetchAllPages<T>(url: string) {
  let nextUrl: string | null = url;
  while (nextUrl) {
    const res = await scraper.get(nextUrl, { responseType: 'json' });
    yield res.body;
    nextUrl = res.headers['link']?.match(/<([^>]+)>;\s*rel="next"/)?.[1] || null;
  }
}
```

### Got Cheat Sheet

```ts
got(url)                    // simple GET
got.get(url, options)       // GET with options
got.post(url, { json: {} }) // POST JSON
got.put()  / got.patch()    // other methods
got(url).text()             // return as string
got(url).json<T>()          // return as JSON
got(url).buffer()           // return as Buffer
got.stream(url)             // streaming
got.extend({ ... })         // create reusable client
```

---

## Cheerio â€“ Data Extraction

### Why Cheerio?
- jQuery-like syntax for server-side parsing
- Zero browser needed â€“ lightning fast
- Perfect for static HTML
- Great with Got for complete scraping

### Installation
```bash
npm install cheerio
```

### Load HTML

```ts
import cheerio from 'cheerio';

const $ = cheerio.load(html);
// or load from Got response
const $ = cheerio.load(await got(url).text());
```

### Core Principle
Always re-wrap elements inside `.each()` â†’ use `$(el)` or `$(this)`

### Basic Selectors

| Selector | Meaning |
|----------|---------|
| `$('h1')` | All `<h1>` tags |
| `$('.price')` | By class |
| `$('#main')` | By ID |
| `$('[data-id]')` | Any attribute |
| `$('[href*="login"]')` | Contains |
| `$('[src$=".jpg"]')` | Ends with |
| `$('[data-index^="10"]')` | Starts with |

### Traversing the DOM (Most Used!)

```ts
$(el).find('.price')         // deep search (most common)
$(el).children('a')          // direct children only
$(el).parent()               // one level up
$(el).parents('.container')  // first matching ancestor
$(el).next()                 // next sibling
$(el).prev()                 // previous sibling
$(el).nextAll('.item')       // all following siblings
$(el).siblings()             // all siblings
$(el).eq(2)                  // 3rd item (0-based)
$(el).first() / .last()      // first or last
$(el).slice(0, 10)           // limit results
```

### Top 10 Real-World Extraction Patterns

#### Pattern 1: Product Cards (Amazon, Shopify)
```ts
$('.product-card').map((i, el) => {
  const $card = $(el);
  return {
    title: $card.find('h2 a').text().trim(),
    price: $card.find('.price').text().replace(/[^0-9.]/g, ''),
    url: $card.find('a').attr('href'),
    image: $card.find('img').attr('src') || $card.find('img').data('src'),
  };
}).get();
```

#### Pattern 2: Table to JSON
```ts
$('table tr').slice(1).map((i, row) => ({
  rank: $(row).find('td').eq(0).text().trim(),
  name: $(row).find('td').eq(1).text().trim(),
  score: parseInt($(row).find('td').eq(2).text()) || 0
})).get();
```

#### Pattern 3: Extract JSON from Script Tag
```ts
const json = $('script[type="application/ld+json"]')
  .first().html();
if (json) {
  const data = JSON.parse(json);
  console.log(data);
}
```

#### Pattern 4: Hacker News / Reddit Style (2-row items)
```ts
$('.athing').each((i, el) => {
  const $item = $(el);
  const $subline = $item.next();

  const title = $item.find('.titleline a').text();
  const points = parseInt($subline.find('.score').text()) || 0;
  const comments = parseInt($subline.find('a').last().text()) || 0;
  
  console.log({ title, points, comments });
});
```

#### Pattern 5: Get All Links with Text Filter
```ts
$('a')
  .filter((i, el) => $(el).text().trim() === 'Next â†’')
  .attr('href');
```

#### Pattern 6: Nested Data (Comments Section)
```ts
$('.comment').map((i, el) => {
  const $comment = $(el);
  return {
    author: $comment.find('.author').text().trim(),
    date: $comment.find('.date').attr('title'),
    text: $comment.find('.text').text().trim(),
    replies: $comment.find('.reply').length
  };
}).get();
```

#### Pattern 7: Extract Attributes with Fallbacks
```ts
const image = $img.attr('src') || 
              $img.attr('data-src') || 
              $img.attr('data-lazy-src') ||
              'default-image.jpg';
```

#### Pattern 8: Clean and Parse Text with Numbers
```ts
const price = $(el).text().match(/[\d.]+/)?.[0];
const discount = parseInt($(el).text().match(/(\d+)%/)?.[1]) || 0;
```

#### Pattern 9: Get Meta Information
```ts
const ogTitle = $('meta[property="og:title"]').attr('content');
const ogImage = $('meta[property="og:image"]').attr('content');
const description = $('meta[name="description"]').attr('content');
```

#### Pattern 10: Remove Noise and Extract Main Content
```ts
$('script, style, noscript, svg, iframe').remove();
const content = $('body').text().trim().replace(/\s+/g, ' ');
```

### Text & Attribute Pro Tricks

```ts
// Clean text
$(el).text().trim().replace(/\s+/g, ' ')

// Extract number from text
$(el).text().match(/[\d.]+/)?.[0]

// data-* attributes
$(el).data('asin')    // automatically reads data-asin

// Make absolute URL from relative
new URL($('a').attr('href') || '', 'https://example.com').href

// Check if element exists
if ($('.element').length) { /* found */ }

// Get HTML
$(el).html()   // innerHTML

// Get outer HTML
$.html(el)

// Clone and modify
const $clone = $(el).clone();
```

### One-Liner Magic

```ts
$('meta[property="og:title"]').attr('content')           // OpenGraph title
$('h1, .title, title').first().text().trim()            // main title
$('a:contains("Next")').attr('href')                    // next page
$('.current').parent().next().find('a').attr('href')    // pagination
$('img').map((i,el) => $(el).attr('src')!).get()        // all images
$('script, style, noscript').remove()                    // clean noise
$('.item').length > 0 ? 'Found' : 'Not found'           // exists check
$('.price').text().replace(/[^0-9.]/g, '')              // extract numbers
```

### Common Bugs & Fixes

```ts
// âŒ WRONG
$('.item').each((i, el) => {
  el.find('.price').text()  // el is raw DOM, no .find()
});

// âœ… CORRECT
$('.item').each((i, el) => {
  const $el = $(el);        // re-wrap!
  $el.find('.price').text();
});

// âœ… OR use "this"
$('.item').each(function () {
  const $this = $(this);    // "this" is already wrapped
  $this.find('.price').text();
});
```

### Cheerio Cheat Sheet

```ts
.find()      // descendants (most used)
.children()  // direct children only
.parent()    // up one level
.next()      // next sibling
.text()      // get text
.html()      // get innerHTML
.attr()      // get/set attribute
.data()      // get data-* attribute
.map().get() // convert to array
.filter()    // filter selection
.eq()        // get by index
.length      // count elements
```

---

## Real-World Examples

### Complete Product Scraper

```ts
import got from 'got';
import cheerio from 'cheerio';
import fs from 'fs';

interface Product {
  title: string;
  price: string;
  url: string;
  image: string;
}

class ProductScraper {
  private baseUrl = 'https://example-shop.com';

  async scrapeProducts(): Promise<Product[]> {
    const html = await got(this.baseUrl).text();
    const $ = cheerio.load(html);
    
    const products: Product[] = [];

    $('.product-item').each((i, el) => {
      const $product = $(el);
      const product: Product = {
        title: $product.find('.product-name').text().trim(),
        price: $product.find('.price').text().replace(/[^0-9.]/g, ''),
        url: new URL(
          $product.find('a').attr('href') || '',
          this.baseUrl
        ).href,
        image: $product.find('img').attr('src') || 'N/A',
      };
      products.push(product);
    });

    return products;
  }

  async saveToJSON(products: Product[]) {
    fs.writeFileSync(
      'products.json',
      JSON.stringify(products, null, 2)
    );
    console.log(`Saved ${products.length} products`);
  }
}

// Usage
const scraper = new ProductScraper();
const products = await scraper.scrapeProducts();
await scraper.saveToJSON(products);
```

### Multi-Page Scraper with Delays

```ts
async function scrapeMultiplePages(baseUrl: string, maxPages = 5) {
  const allItems = [];

  for (let page = 1; page <= maxPages; page++) {
    const url = `${baseUrl}?page=${page}`;
    console.log(`Scraping page ${page}...`);
    
    try {
      const html = await scraper.get(url).text();
      const $ = cheerio.load(html);
      
      const items = $('.item').map((i, el) => ({
        title: $(el).find('h2').text().trim(),
        desc: $(el).find('p').text().trim(),
      })).get();

      if (items.length === 0) break; // Stop if empty page
      
      allItems.push(...items);
      
      // Be respectful â€“ add delay between requests
      await new Promise(r => setTimeout(r, 2000 + Math.random() * 3000));
    } catch (err) {
      console.error(`Error on page ${page}:`, err);
    }
  }

  return allItems;
}
```

---

## Cheat Sheets

### Got Quick Reference

```ts
// Methods
got(url)
got.get(url)
got.post(url, { json: {} })
got.put(url)
got.delete(url)
got.patch(url)

// Response formats
.text()              // string
.json<T>()           // parsed JSON
.buffer()            // Buffer
.stream()            // stream

// Common options
{ timeout: 10000 }
{ retry: { limit: 3 } }
{ followRedirect: true }
{ throwHttpErrors: false }
{ headers: { ... } }
```

### Cheerio Quick Reference

```ts
// Load
cheerio.load(html)

// Select
$('selector')
$('.class')
$('#id')
$('[attr]')

// Traverse
.find()              // descendants
.children()          // direct children
.parent()            // up
.next()              // next sibling
.prev()              // previous sibling

// Extract
.text()              // text content
.html()              // inner HTML
.attr('name')        // attribute
.data('key')         // data attribute
.prop('checked')     // property

// Array operations
.map().get()         // to array
.eq(index)           // by index
.first() / .last()   // boundaries
.slice(start, end)   // range
.filter(fn)          // filter
```

### Anti-Bot Bypass Headers

```ts
const headers = {
  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Accept-Language': 'en-US,en;q=0.5',
  'Accept-Encoding': 'gzip, deflate, br',
  'Connection': 'keep-alive',
  'Upgrade-Insecure-Requests': '1',
  'Cache-Control': 'max-age=0',
};
```

### Cookie Management (Session Persistence)

```ts
import got from 'got';
import { CookieJar } from 'tough-cookie';
import { HttpsCookieAgent } from 'http-cookie-agent/http';

// 1. Auto cookie handling (most common)
const cookieJar = new CookieJar();
const client = got.extend({
  cookieJar,
  agent: {
    https: new HttpsCookieAgent({ cookies: { jar: cookieJar } })
  }
});

// Login â†’ cookies saved automatically
await client.post('https://example.com/login', {
  form: {
    username: 'myuser',
    password: 'mypass123'
  }
});

// Now youâ€™re logged in â€“ all future requests include cookies!
const profile = await client.get('https://example.com/my-profile').text();
console.log('Logged in as:', profile.match(/Welcome, (.+)!/)?.[1]);

// 2. Manual cookie setting (for Cloudflare, bot detection bypass)
await client.get('https://httpbin.org/cookies/set', {
  searchParams: { session_id: 'abc123', cf_clearance: 'xyz...' }
});

// 3. Bearer Token + CookieJar (modern apps)
const authClient = got.extend({
  cookieJar,
  headers: {
    Authorization: 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6...'
  }
});
```
### Request Caching (Avoid Duplicate Requests)
```ts
import got from 'got';
import { CacheableRequest } from 'cacheable-request';

const scraper = got.extend({
  cache: new Map(), // In-memory cache
  cacheOptions: {
    shared: false,
    cacheHeuristic: 0.1,
    immutableMinTimeToLive: 24 * 60 * 60 * 1000, // 24 hours
    ignoreCargoCult: false
  }
});

// First request hits the server
await scraper('https://example.com');

// Second request uses cache
await scraper('https://example.com');
```
### Full Advanced Scraper Class with All Features
```ts
import got from 'got';
import cheerio from 'cheerio';
import { CookieJar } from 'tough-cookie';
import { HttpsCookieAgent } from 'http-cookie-agent/http';
import fs from 'fs';
import { stringify } from 'csv-stringify/sync';

class UltimateScraper {
  private client = got.extend({
    timeout: { request: 20000 },
    retry: { limit: 3 },
    followRedirect: true,
    cookieJar: new CookieJar(),
    agent: {
      https: new HttpsCookieAgent({ cookies: { jar: new CookieJar() } })
    },
    headers: {
      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)...',
      'Accept-Language': 'en-US,en;q=0.9'
    }
  });

  async login(username: string, password: string) {
    await this.client.post('https://site.com/login', {
      form: { username, password }
    });
    console.log('Logged in!');
  }

  async scrapeAndSave(url: string, filename = 'data.csv') {
    const html = await this.client.get(url).text();
    const $ = cheerio.load(html);

    const data = $('.item').map((i, el) => ({
      title: $(el).find('h2').text().trim(),
      price: parseFloat($(el).find('.price').text().replace('$', '')) || 0,
      url: $(el).find('a').attr('href')
    })).get();

    const csv = stringify(data, { header: true });
    fs.writeFileSync(filename, csv);
    console.log(`Saved ${data.length} items â†’ ${filename}`);
  }
}
```

---

## Best Practices

1. Always check `robots.txt` and Terms of Service
2. Add random delays: `await delay(1000 + Math.random() * 2000)`
3. Use residential proxies for tough sites
4. Handle 403/429 errors with retries + proxy rotation
5. Never scrape personal data (GDPR/CCPA compliance)
6. Set meaningful timeouts
7. Log your requests for debugging
8. Test selectors in browser console first
9. Handle empty results gracefully
10. Cache responses when possible

---

Happy scraping! ðŸš€
