## First: Install required dependencies
```bash
npm install csv-parser csv-writer
npm install --save-dev @types/node
npm install csv-stringify # lightweight built-in + popular libraries:
```
## Text File Operations (.txt)
```ts
// Modern, clean imports (no * as anywhere)
import fs from 'fs/promises';
import path from 'path';

// Ensure directory exists
async function ensureDir(filePath: string): Promise<void> {
  await fs.mkdir(path.dirname(filePath), { recursive: true });
}

// Write (overwrite) a text file
export async function writeTextFile(filePath: string, content: string): Promise<void> {
  await ensureDir(filePath);
  await fs.writeFile(filePath, content, 'utf-8');
}

// Append to a text file (adds newline automatically)
export async function appendTextFile(filePath: string, content: string): Promise<void> {
  await ensureDir(filePath);
  await fs.appendFile(filePath, content + '\n', 'utf-8');
}

// Read entire text file as string
export async function readTextFile(filePath: string): Promise<string> {
  return await fs.readFile(filePath, 'utf-8');
}

// Read file line by line (proper streaming version – much better for large files)
export async function readTextFileLines(filePath: string): AsyncIterable<string> {
  const fileHandle = await fs.open(filePath, 'r');
  const stream = fileHandle.createReadStream({ encoding: 'utf-8' });

  let buffer = '';
  for await (const chunk of stream) {
    buffer += chunk;
    let lineEnd: number;
    while ((lineEnd = buffer.indexOf('\n')) !== -1) {
      let line = buffer.slice(0, lineEnd);
      buffer = buffer.slice(lineEnd + 1);
      // Handle Windows CRLF
      if (line.endsWith('\r')) line = line.slice(0, -1);
      if (line.trim() !== '') yield line;
    }
  }

  // Yield remaining line if file doesn't end with newline
  if (buffer.trim() !== '') yield buffer.trim();

  await fileHandle.close();
}
```
## Define your data type (example) for csv
```ts
interface Person {
  name: string;
  age: number;
  city: string;
}
```
## Write CSV (create or overwrite)
```ts
import { createObjectCsvWriter } from 'csv-writer';

export async function writeCsvFile<T extends Record<string, any>>(
  filePath: string,
  records: T[],
  header: { id: keyof T; title: string }[]
): Promise<void> {
  await ensureDir(filePath);

  const csvWriter = createObjectCsvWriter({
    path: filePath,
    header,
  });

  await csvWriter.writeRecords(records);
}
```
## Append to existing CSV
```ts
import { createObjectCsvWriter } from 'csv-writer';

export async function appendToCsvFile<T extends Record<string, any>>(
  filePath: string,
  records: T[],
  header: { id: keyof T; title: string }[]
): Promise<void> {
  await ensureDir(filePath);

  const csvWriter = createObjectCsvWriter({
    path: filePath,
    header,
    append: true, // This enables append mode
  });

  await csvWriter.writeRecords(records);
}
```
## Read CSV File
```ts
import { createReadStream, existsSync } from 'fs';
import csvParser from 'csv-parser';
import { pipeline } from 'stream/promises';

export const readCsvFile = async <T = any>(filePath: string): Promise<T[]> => {
  if (!existsSync(filePath)) return [];

  const results: T[] = [];

  await pipeline(
    createReadStream(filePath),
    csvParser(),
    async function* (source) {
      for await (const row of source) results.push(row as T);
    }
  );

  return results;
};
```
## Example Usage
```ts
async function main() {
  const txtFile = './data/log.txt';
  const csvFile = './data/people.csv';

  // Text file examples
  await writeTextFile(txtFile, 'Hello TypeScript!\n');
  await appendTextFile(txtFile, 'Second line');
  await appendTextFile(txtFile, 'Third line');

  const content = await readTextFile(txtFile);
  console.log('Text file content:\n', content);

  // CSV examples
  const header = [
    { id: 'name', title: 'Name' },
    { id: 'age', title: 'Age' },
    { id: 'city', title: 'City' },
  ];

  const people: Person[] = [
    { name: 'Alice', age: 25, city: 'New York' },
    { name: 'Bob', age: 30, city: 'London' },
  ];

  await writeCsvFile(csvFile, people, header);
  await appendToCsvFile(csvFile, [{ name: 'Charlie', age: 35, city: 'Paris' }], header);

  const loadedPeople = await readCsvFile<Person>(csvFile);
  console.log('CSV data:', loadedPeople);
}

main().catch(console.error);
```
## Lightweight CSV without external deps (using csv-parse/stringify)
```ts
import { parse } from 'csv-parse/sync';
import { stringify } from 'csv-stringify/sync';
import fs from 'fs/promises';
import path from 'path';

// Optional: auto-create directories (like your previous helpers)
const ensureDir = (filePath: string) =>
  fs.mkdir(path.dirname(filePath), { recursive: true });

export async function writeCsvSimple(filePath: string, data: any[][]): Promise<void> {
  await ensureDir(filePath);
  const csv = stringify(data, { header: true });
  await fs.writeFile(filePath, csv, 'utf-8');
}
```
## Real-World Scraper Example
```ts
// scraper.ts
import { appendCSVRow, readAndLoopCSV, writeCSV, readCSV } from './csv';

interface Product {
  title: string;
  price: string;
  url: string;
  date: string;
}

const FILE = './output/products.csv';

async function scrapeAndSave() {
  console.log('Scraping started...');

  // Example: scraping 5 products one by one
  const products = [
    { title: 'iPhone 16', price: '$999', url: 'https://apple.com/1', date: new Date().toISOString() },
    { title: 'Samsung S24', price: '$899', url: 'https://samsung.com/1', date: new Date().toISOString() },
    { title: 'MacBook Pro', price: '$2499', url: 'https://apple.com/2', date: new Date().toISOString() },
  ];

  for (const product of products) {
    await appendCSVRow(FILE, product);  // Appends 1 row at a time → safe & fast
    console.log(`Saved: ${product.title}`);
    // await fakeNetworkDelay(); // simulate real scraping delay
  }

  console.log('All saved!');
}

async function showAllData() {
  console.log('\nReading all saved products:');
  const data = await readCSV<Product>(FILE);
  console.table(data);
}

async function processEachRow() {
  console.log('\nProcessing each row one by one:');
  await readAndLoopCSV<Product>(FILE, (row, index) => {
    console.log(`${index + 1}. ${row.title} → ${row.price}`);
  });
}

// Run examples
async function main() {
  await scrapeAndSave();
  await showAllData();
  await processEachRow();

  // Or overwrite everything
  // await writeCSV(FILE, [{ title: 'New Phone', price: '$100', url: '...', date: '...' }]);
}

main().catch(console.error);
```

## Handle JSON file
```ts
// example-usage.ts
import { readJson, writeJson, appendJsonArray, updateJson } from './json-utils';

interface Product {
  title: string;
  price: string;
  url: string;
  scrapedAt: string;
}

const PRODUCTS_FILE = 'data/products.json';
const PROGRESS_FILE = 'data/scrape-progress.json';

// Fake data
const fakeProducts: Product[] = [
  { title: "iPhone 16", price: "$999", url: "https://apple.com/1", scrapedAt: new Date().toISOString() },
  { title: "AirPods Pro", price: "$249", url: "https://apple.com/2", scrapedAt: new Date().toISOString() },
];

// ─────────────────────────────────────
// 1. Save scraped items one by one (append mode)
async function scrapeAndAppend() {
  console.log("Scraping and appending one by one...");
  for (const product of fakeProducts) {
    await appendJsonArray(PRODUCTS_FILE, product);
    console.log(`Appended: ${product.title}`);
    // await delay(500); // simulate network
  }
}

// ─────────────────────────────────────
// 2. Track scraping progress (resume support!)
async function markProgress(page: number) {
  await updateJson(PROGRESS_FILE, (prev) => ({
    lastPage: page,
    totalProcessed: (prev.totalProcessed || 0) + 1,
    resumed: prev.resumed || false,
    lastRun: new Date().toISOString(),
  }), { lastPage: 0, totalProcessed: 0 });
}

// ─────────────────────────────────────
// 3. Read and show all saved data
async function showAll() {
  const products = await readJson<Product[]>(PRODUCTS_FILE, []);
  console.log(`\nFound ${products.length} products:`);
  console.table(products.map(p => ({ title: p.title, price: p.price })));

  const progress = await readJson(PROGRESS_FILE, { lastPage: 0 });
  console.log("Progress:", progress);
}

// ─────────────────────────────────────
// Run everything
async function main() {
  await scrapeAndAppend();
  await markProgress(42);
  await showAll();

  // Example: Force resume from page 201 next time
  await writeJson(PROGRESS_FILE, { lastPage: 200, totalProcessed: 1000, resumed: true });
  console.log("Forced next run to resume from page 201");
}

main().catch(console.error);
```

