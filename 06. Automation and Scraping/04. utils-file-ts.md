## Install
```bash
npm install csv-stringify csv-parse
npm install --save-dev @types/node
```
## 00. utils/progress.ts
```ts
// utils/progress.ts
import fs from 'fs/promises';
import path from 'path';

export async function ensureDir(filePath: string): Promise<void> {
  const dir = path.dirname(filePath);
  if (dir) {
    await fs.mkdir(dir, { recursive: true });
  }
}

// Save current page (we only need currentPage — totalPages is optional)
export async function saveProgress(PROGRESS_FILE: string, currentPage: number): Promise<void> {
  const progress = {
    currentPage,
    savedAt: new Date().toISOString(),
  };
  await ensureDir(PROGRESS_FILE);
  await fs.writeFile(PROGRESS_FILE, JSON.stringify(progress, null, 2), 'utf-8');
}

// Load progress — returns the NEXT page to scrape
// If file doesn't exist or invalid → starts from page 1
export async function loadProgress(
  totalPages: number,
  PROGRESS_FILE: string
): Promise<number> {
  try {
    const data = await fs.readFile(PROGRESS_FILE, 'utf-8');
    const json = JSON.parse(data);

    // Validate that currentPage is a valid number and in range
    if (
      typeof json.currentPage === 'number' &&
      json.currentPage >= 1 &&
      json.currentPage <= totalPages
    ) {
      const nextPage = json.currentPage + 1;
      console.log(`Resuming from page ${nextPage} (last saved: ${json.currentPage})`);
      return nextPage;
    }
  } catch (err: any) {
    // ENOENT = file not found → normal, just start from 1
    // JSON parse error → corrupted → start from 1
    if (err.code !== 'ENOENT') {
      console.warn('Progress file corrupted or invalid → starting from page 1');
    }
  }

  // No valid progress → start from page 1
  console.log('No progress found → starting from page 1');
  return 1;
}
```

## 01. utils/text.ts – Text File Utils
```ts
import fs from 'fs/promises';
import path from 'path';

// ─────────────────────────────────────
// Shared: Ensure directory exists
export async function ensureDir(filePath: string): Promise<void> {
  await fs.mkdir(path.dirname(filePath), { recursive: true });
}
// ─────────────────────────────────────
// Progress Management
export interface Progress {
  currentPage: number;
  totalPages?: number;
  lastRun?: string;
}
// ─────────────────────────────────────
// Text file operations
export async function writeTextFile(filePath: string, content: string): Promise<void> {
  await ensureDir(filePath);
  await fs.writeFile(filePath, content, 'utf-8');
}

export async function appendTextFile(filePath: string, content: string): Promise<void> {
  await ensureDir(filePath);
  await fs.appendFile(filePath, content + '\n', 'utf-8');
}

export async function readTextFile(filePath: string): Promise<string> {
  return await fs.readFile(filePath, 'utf-8');
}
```

## 02. utils/csv.ts – Fast CSV Utils (2025 Standard)
```ts
import fs from 'fs/promises';
import path from 'path';
import { stringify } from 'csv-stringify/sync';
import { parse } from 'csv-parse/sync'; // ← sync is best for reading

// ─────────────────────────────────────
// Shared: Ensure dir + Progress
export async function ensureDir(filePath: string): Promise<void> {
  await fs.mkdir(path.dirname(filePath), { recursive: true });
}

export interface Progress {
  currentPage: number;
  lastRun?: string;
}
// ─────────────────────────────────────
// CSV: Write / Overwrite
export async function writeCsvFile<T extends Record<string, any>>(
  filePath: string,
  records: T[],
  header: { id: keyof T; title: string }[]
): Promise<void> {
  await ensureDir(filePath);
  const columns = header.map(h => ({ key: h.id as string, header: h.title }));
  const csv = stringify(records, { header: true, columns });
  await fs.writeFile(filePath, csv, 'utf-8');
}

// ─────────────────────────────────────
// CSV: Append
export async function appendToCsvFile<T extends Record<string, any>>(
  filePath: string,
  records: T[],
  header: { id: keyof T; title: string }[]
): Promise<void> {
  await ensureDir(filePath);
  const fileExists = await fs.stat(filePath).then(() => true).catch(() => false);
  const columns = header.map(h => ({ key: h.id as string, header: h.title }));
  const csv = stringify(records, { header: !fileExists, columns });
  await fs.appendFile(filePath, csv, 'utf-8');
}

// ─────────────────────────────────────
// CSV: Read – SIMPLE, FAST, NO ERRORS, 2025 STANDARD
export async function readCsvFile<T extends Record<string, any>>(
  filePath: string
): Promise<T[]> {
  try {
    const content = await fs.readFile(filePath, 'utf-8');

    // This is the correct return — NOT the parser!
    return parse(content, {
      columns: true,
      skip_empty_lines: true,
      trim: true,
    }) as T[];
  } catch (err: any) {
    if (err.code === 'ENOENT') {
      return []; // File doesn't exist → return empty array
    }
    throw err; // Other errors (corrupted file, etc.)
  }
}
```
## 03. utils/json.ts – JSON Utils (Perfect for Scrapers)
```ts
import fs from 'fs/promises';
import path from 'path';

// ─────────────────────────────────────
// Shared: Ensure dir + Progress
export async function ensureDir(filePath: string): Promise<void> {
  await fs.mkdir(path.dirname(filePath), { recursive: true });
}

// ─────────────────────────────────────
// JSON Helpers
export async function readJson<T>(filePath: string, defaultValue: T): Promise<T> {
  try {
    const data = await fs.readFile(filePath, 'utf-8');
    return JSON.parse(data) as T;
  } catch (err: any) {
    if (err.code === 'ENOENT') return defaultValue;
    throw err;
  }
}

export async function writeJson<T>(filePath: string, data: T): Promise<void> {
  await ensureDir(filePath);
  await fs.writeFile(filePath, JSON.stringify(data, null, 2), 'utf-8');
}

export async function appendJsonArray<T>(filePath: string, item: T): Promise<void> {
  const arr = await readJson<T[]>(filePath, []);
  arr.push(item);
  await writeJson(filePath, arr);
}

export async function updateJson<T extends object>(
  filePath: string,
  updater: (current: T) => T,
  defaultValue: T
): Promise<void> {
  const current = await readJson(filePath, defaultValue);
  await writeJson(filePath, updater(current));
}
```

## Example Uses (Scrap all links by pagination)
```ts
import got from 'got';
import * as cheerio from 'cheerio';
import fs from 'fs/promises';
import path from 'path';

const OUTPUT_FILE = 'data/notaires_all_links.txt';
const PROGRESS_FILE = 'data/scrape_progress.json';  // ← NEW: tracks progress

async function ensureDir(filePath: string): Promise<void> {
  await fs.mkdir(path.dirname(filePath), { recursive: true });
}

async function appendTextFile(filePath: string, content: string): Promise<void> {
  await ensureDir(filePath);
  await fs.appendFile(filePath, content + '\n', 'utf-8');
}

// Save progress to disk
async function saveProgress(currentPage: number, totalPages: number) {
  await ensureDir(PROGRESS_FILE);
  await fs.writeFile(PROGRESS_FILE, JSON.stringify({ currentPage, totalPages }, null, 2));
}

// Load progress (returns start page)
async function loadProgress(totalPages: number): Promise<number> {
  try {
    const data = await fs.readFile(PROGRESS_FILE, 'utf-8');
    const json = JSON.parse(data);
    if (json.currentPage >= 1 && json.currentPage <= totalPages) {
      console.log(`Resuming from page ${json.currentPage + 1}`);
      return json.currentPage + 1;
    }
  } catch (err) {
    // No progress file → start from beginning
  }
  return 1;
}

// Your existing getLinks function (unchanged)
const getLinks = async (url: string): Promise<string[]> => {
  const response = await got(url, {
    headers: {
      'User-Agent': 'Mozilla/5.0 (compatible; NotaryDirectoryScraper/1.0)',
    },
    timeout: { request: 15000 },
    retry: { limit: 3 },
  });

  if (response.statusCode !== 200) {
    throw new Error(`Status ${response.statusCode}`);
  }

  const $ = cheerio.load(response.body);
  const links: string[] = [];

  $('h2.notary-card__title a').each((_, el) => {
    const href = $(el).attr('href');
    if (href) links.push(`https://www.notaires.fr${href}`);
  });

  return links;
};

// MAIN SCRAPER — NOW RESUMABLE
async function scrapeAllPages() {
  const totalPages = 1932;
  const concurrency = 3;

  // ←←←←←←←←←←←←←←←←←←←←←←←←←←←←←←
  // JUST CHANGE THIS NUMBER TO RESUME FROM ANYWHERE
  let startPage = await loadProgress(totalPages);
  // Example: force start from page 201
  // startPage = 201;
  // ←←←←←←←←←←←←←←←←←←←←←←←←←←←←←←

  const endPage = totalPages;
  const pagesToScrape = Array.from({ length: endPage - startPage + 1 }, (_, i) => i + startPage);

  console.log(`Starting from page ${startPage} → ${endPage} (${pagesToScrape.length} pages)`);

  let processed = 0;

  const worker = async (page: number) => {
    const url = page === 1
      ? 'https://www.notaires.fr/fr/directory/notaries'
      : `https://www.notaires.fr/fr/directory/notaries?page=${page}`;

    console.log(`Fetching page ${page}...`);

    try {
      const links = await getLinks(url);
      for (const link of links) {
        await appendTextFile(OUTPUT_FILE, link);
      }
      console.log(`Page ${page} success (${links.length} links)`);

      // ←←← UPDATE PROGRESS ON SUCCESS
      await saveProgress(page, totalPages);
      // ←←←←←←←←←←←←←←←←←←←←←←←←←←←←←←

    } catch (err: any) {
      console.error(`Page ${page} failed:`, err.message);
      // Optional: save failed pages
      await appendTextFile('data/failed_pages.txt', `Page ${page}: ${err.message}`);
      // Do NOT update progress on failure → next run retries this page
    } finally {
      processed++;
      console.log(`Progress: ${processed}/${pagesToScrape.length} | Overall: ${page}/${totalPages}`);
    }

    // Be nice to the server
    await new Promise(r => setTimeout(r, 800 + Math.random() * 700));
  };

  // Concurrency control
  const batch: Promise<void>[] = [];
  for (const page of pagesToScrape) {
    batch.push(worker(page));
    if (batch.length >= concurrency) {
      await Promise.all(batch.splice(0));
    }
  }
  await Promise.all(batch);

  console.log('Scraping completed!');
  // Optional: delete progress file when done
  await fs.unlink(PROGRESS_FILE).catch(() => {});
}

scrapeAllPages().catch(console.error);
```
## Example Uses (Scrap Details Page data)
```ts
// scraper.ts
import got from 'got';
import * as cheerio from 'cheerio';
import fs from 'fs/promises';
import { appendToCsvFile } from './utils/csv.js';
import { loadProgress, saveProgress } from './utils/progress.js';

const LINKS_FILE = 'data/notaires_all_links.txt';
const PROGRESS_FILE = 'data/notaires_progress.json';
const OUTPUT_FILE = 'data/notaires_data.csv';
const CONCURRENCY = 3; // Number of URLs to process simultaneously

// Define the type
type NotaryField = 'name' | 'phone' | 'languages' | 'website' | 'status' | 'profileUrl';

// CSV Header (only written on first run)
const HEADER: Array<{ id: NotaryField; title: string }> = [
  { id: 'name', title: 'Full Name' },
  { id: 'phone', title: 'Phone' },
  { id: 'languages', title: 'Languages' },
  { id: 'website', title: 'Website' },
  { id: 'status', title: 'Status' },
  { id: 'profileUrl', title: 'Profile URL' },
];

// Single notary scraper
async function scrapeNotary(url: string): Promise<void> {
  try {
    const response = await got(url, {
      headers: {
        'User-Agent':
          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',
      },
      timeout: { request: 15000 },
      retry: { limit: 3 },
    });

    const $ = cheerio.load(response.body);

    const name = $('div.notary-card__line').first().text().trim() || 'N/A';
    const phone = $('div.office-sheet__phone.field--telephone').first().text().trim() || 'N/A';
    const languages = $('div.field__items.field-spoken-languages').first().text().trim() || 'N/A';
    const website = $('div.office-sheet__url.field--link a').first().attr('href')?.trim() || '';
    const status = $('p.notary-card__status').first().text().trim() || 'N/A';

    const row = {
      name,
      phone,
      languages,
      website,
      status,
      profileUrl: url,
    };

    await appendToCsvFile(OUTPUT_FILE, [row], HEADER);
    console.log(`Saved: ${name}`);
  } catch (err: any) {
    console.error(`Failed: ${url} → ${err.message}`);
    // Still count as processed so we don't retry forever
  }
}

// MAIN SCRAPER — RESUMABLE & SAFE
async function main() {
  console.log('Loading links from:', LINKS_FILE);
  const content = await fs.readFile(LINKS_FILE, 'utf-8');
  const allLinks = content
    .split('\n')
    .map(l => l.trim())
    .filter(l => l.startsWith('http'));

  const total = allLinks.length;
  if (total === 0) {
    console.log('No links found!');
    return;
  }

  let startIndex = await loadProgress(total, PROGRESS_FILE);
  console.log(`Starting from link ${startIndex} of ${total}`);

  for (let i = startIndex - 1; i < allLinks.length; i += CONCURRENCY) {
    // Process URLs concurrently based on CONCURRENCY setting
    const batch = allLinks
      .slice(i, i + CONCURRENCY)
      .filter((url): url is string => Boolean(url)); // Type guard to remove undefined

    if (batch.length === 0) continue;

    console.log(`\n[${i + 1}-${i + batch.length}/${total}] Processing batch of ${batch.length} URLs`);

    // Scrape all URLs in batch concurrently
    await Promise.all(batch.map(url => scrapeNotary(url)));

    // Save progress after batch completes
    const currentIndex = i + batch.length;
    await saveProgress(PROGRESS_FILE, currentIndex);
  }

  console.log('\nAll done! Scraping completed successfully!');
  console.log(`Data saved to: ${OUTPUT_FILE}`);

  // Optional: remove progress file when fully done
  await fs.unlink(PROGRESS_FILE).catch(() => {});
}

main().catch(err => {
  console.error('Scraper crashed:', err);
  process.exit(1);
});
```
## Example Uses (Details page data scrap - one by one)
```ts
// scraper.ts
import got from 'got';
import * as cheerio from 'cheerio';
import fs from 'fs/promises';
import { appendToCsvFile } from './utils/csv.js';
import { loadProgress, saveProgress } from './utils/progress.js';

const LINKS_FILE = 'data/notaires_all_links.txt';
const PROGRESS_FILE = 'data/notaires_progress.json';
const OUTPUT_FILE = 'data/notaires_data.csv';

// Define the type
type NotaryField = 'name' | 'phone' | 'languages' | 'website' | 'status' | 'profileUrl';

// CSV Header (only written on first run)
const HEADER: Array<{ id: NotaryField; title: string }> = [
  { id: 'name', title: 'Full Name' },
  { id: 'phone', title: 'Phone' },
  { id: 'languages', title: 'Languages' },
  { id: 'website', title: 'Website' },
  { id: 'status', title: 'Status' },
  { id: 'profileUrl', title: 'Profile URL' },
];

// Single notary scraper
async function scrapeNotary(url: string): Promise<void> {
  try {
    const response = await got(url, {
      headers: {
        'User-Agent':
          'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',
      },
      timeout: { request: 15000 },
      retry: { limit: 3 },
    });

    const $ = cheerio.load(response.body);

    const name = $('div.notary-card__line').first().text().trim() || 'N/A';
    const phone = $('div.office-sheet__phone.field--telephone').first().text().trim() || 'N/A';
    const languages = $('div.field__items.field-spoken-languages').first().text().trim() || 'N/A';
    const website = $('div.office-sheet__url.field--link a').first().attr('href')?.trim() || '';
    const status = $('p.notary-card__status').first().text().trim() || 'N/A';

    const row = {
      name,
      phone,
      languages,
      website,
      status,
      profileUrl: url,
    };

    await appendToCsvFile(OUTPUT_FILE, [row], HEADER);
    console.log(`Saved: ${name}`);
  } catch (err: any) {
    console.error(`Failed: ${url} → ${err.message}`);
    // Still count as processed so we don't retry forever
  }
}

// MAIN SCRAPER — RESUMABLE & SAFE
async function main() {
  console.log('Loading links from:', LINKS_FILE);
  const content = await fs.readFile(LINKS_FILE, 'utf-8');
  const allLinks = content
    .split('\n')
    .map(l => l.trim())
    .filter(l => l.startsWith('http'));

  const total = allLinks.length;
  if (total === 0) {
    console.log('No links found!');
    return;
  }

  let startIndex = await loadProgress(total, PROGRESS_FILE);
  console.log(`Starting from link ${startIndex} of ${total}`);

  for (let i = startIndex - 1; i < allLinks.length; i++) {
    const url = allLinks[i];
    if (!url) continue; // Skip if undefined
    
    const currentIndex = i + 1;

    console.log(`\n[${currentIndex}/${total}] Scraping: ${url}`);
    await scrapeNotary(url);

    // Save progress after every successful or failed link
    await saveProgress(PROGRESS_FILE, currentIndex);
  }

  console.log('\nAll done! Scraping completed successfully!');
  console.log(`Data saved to: ${OUTPUT_FILE}`);

  // Optional: remove progress file when fully done
  await fs.unlink(PROGRESS_FILE).catch(() => {});
}

main().catch(err => {
  console.error('Scraper crashed:', err);
  process.exit(1);
});
```

